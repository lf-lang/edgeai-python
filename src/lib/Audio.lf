/**
 * @file Audio.lf
 * @brief File containing Library of Reactor Components for Audio tasks. This library provides a set
 * of reusable and optimized components for performing various audio tasks, such as audio
 * classification, with high performance and efficiency.
 *
 * @author Vincenzo Barbuto
 */
target Python

import MLReactor from "../lib/private/AbstractReactors.lf"

/**
 * @brief The `AudioClassifier` reactor extends the `MLReactor` to perform audio classification tasks 
 * using a TensorFlow Lite model. It processes input audio data and produces classification results 
 * along with inference time.
 * 
 * This reactor leverages the TensorFlow Lite Support Library for audio classification, offering configurable 
 * options such as model path, maximum results, and score thresholds. It ensures efficient handling of input 
 * audio and outputs structured classification results.
 * 
 * Args:
 *  - `model` (str): The path to the TensorFlow Lite model file. Required.
 *  - `max_results` (int): The maximum number of classification results to return. Defaults to a library-defined value.
 *  - `score_threshold` (float): The minimum score required to include a classification in the results. Defaults to a library-defined value.
 *  - `num_threads` (int): The number of CPU threads to use for inference.
 *  - `enable_edgetpu` (bool): Whether to use the Edge TPU for inference.
 *  - `debug` (bool): Enables debug logging for operations. Defaults to true.
 * 
 * Inputs:
 *  - `input_data`: The input audio data to be classified.
 * 
 * Outputs:
 *  - `results`: A list of classification results, each containing:
 *    - `index`: The index of the classified category.
 *    - `label`: The category name.
 *    - `score`: The classification confidence score.
 *    - `head`: The name of the classification head.
 *  - `inference_time`: The time taken for inference in milliseconds.
 * 
 * Reactions:
 *  - `startup`: 
 *    - Initializes the audio classifier with the provided model path and classification options.
 *    - Validates that a model path is provided; otherwise, logs an error and requests termination.
 *    - Prepares the input tensor for audio classification.
 *  - `input_data`:
 *    - If input data is present, loads the data into the classifier's input tensor and runs inference.
 *    - Captures the time taken for inference and outputs it through `inference_time`.
 *    - Collects and formats the classification results using the `collect_results` method, then outputs them through `results`.
 *    - Logs an error and requests termination if input data is missing.
 *  - `shutdown`:
 *    - Logs the shutdown event in debug mode.
 * 
 * Helper Functions:
 *  - `collect_results`: Processes and formats classification results into a list of dictionaries. Each entry contains:
 *    - `index`: The index of the top category.
 *    - `label`: The name of the top category.
 *    - `score`: The confidence score of the top category.
 *    - `head`: The classification head name.
 * 
 */
reactor AudioClassifier extends MLReactor {
  preamble {=
    from tflite_support.task import audio, core, processor

    def collect_results(self, classifications):
        results_list = [
            {
                "index": classification.categories[0].index if classification.categories else None,
                "label": classification.categories[0].category_name if classification.categories else None,
                "score": classification.categories[0].score if classification.categories else None,
                "head": classification.head_name
            }
            for classification in classifications if classification
        ]
        return results_list
  =}

  reaction(startup) {=
    if self.model == "":
        self.debug and print("[AUDIO CLASSIFIER] Error: Please provide a valid model path")
        lf.request_stop()
    else:
        classification_options = self.processor.ClassificationOptions(
            max_results=self.max_results, score_threshold=self.score_threshold)
        options = self.audio.AudioClassifierOptions(
            base_options=self.base_options, classification_options=classification_options)
        self.executor = self.audio.AudioClassifier.create_from_options(options)

        self.tensor_input = self.executor.create_input_tensor_audio()
  =}

  reaction(input_data) -> results, inference_time {=
    if input_data.is_present and input_data.value is not None:
        # Run inference
        self.tensor_input.load_from_array(input_data.value)
        # Load the input audio and run classify.
        start = lf.time.physical()
        result = self.executor.classify(self.tensor_input)
        end = (lf.time.physical() - start) / 1000000
        results.set(self.collect_results(result.classifications))
        inference_time.set(end)
  =}

  reaction(shutdown) {=
    self.debug and print("[AUDIO CLASSIFIER] Shutting down AudioClassifier reactor")
  =}
}
