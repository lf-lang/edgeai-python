/**
 * @file ComputerVision.lf
 * @brief File containing Library of Reactor Components for Computer Vision tasks. This library
 * provides a set of reusable components for performing various computer vision tasks, such as image
 * classification, object detection, and image segmentation
 *
 * @author Vincenzo Barbuto
 */
target Python

import MLReactor from "../lib/private/AbstractReactors.lf"

/**
 * @brief The `ImageClassifier` reactor extends the `MLReactor` to perform image classification tasks 
 * using a TensorFlow Lite model. It processes input images and produces classification results 
 * along with inference time.
 * 
 * This reactor leverages the TensorFlow Lite Support Library for image classification, offering configurable 
 * options such as model path, maximum results, and score thresholds. It efficiently handles input images and 
 * outputs structured classification results.
 * 
 * Args:
 *  - `model` (str): The path to the TensorFlow Lite model file. Required.
 *  - `max_results` (int): The maximum number of classification results to return. Defaults to a library-defined value.
 *  - `score_threshold` (float): The minimum score required to include a classification in the results. Defaults to a library-defined value.
 *  - `num_threads` (int): Number of CPU threads to use for inference.
 *  - `enable_edgetpu` (bool): Whether to use the Edge TPU for inference.
 *  - `debug` (bool): Enables debug logging for operations. Defaults to true.
 * 
 * Inputs:
 *  - `input_data`: The input image data to be classified.
 * 
 * Outputs:
 *  - `results`: A list of classification results, each containing:
 *    - `index`: The index of the classified category.
 *    - `label`: The category name.
 *    - `score`: The classification confidence score.
 *    - `head`: The name of the classification head.
 *  - `inference_time`: The time taken for inference in milliseconds.
 * 
 * Reactions:
 *  - `startup`: 
 *    - Initializes the image classifier with the provided model path and classification options.
 *    - Validates that a model path is provided; otherwise, logs an error and requests termination.
 *  - `input_data`:
 *    - If input data is present, loads the data into the classifier's input tensor and runs inference.
 *    - Captures the time taken for inference and outputs it through `inference_time`.
 *    - Collects and formats the classification results using the `collect_results` method, then outputs them through `results`.
 *    - Logs an error and requests termination if input data is missing.
 *  - `shutdown`:
 *    - Logs the shutdown event in debug mode.
 * 
 * Helper Functions:
 *  - `collect_results`: Processes and formats classification results into a list of dictionaries. Each entry contains:
 *    - `index`: The index of the top category.
 *    - `label`: The name of the top category.
 *    - `score`: The confidence score of the top category.
 *    - `head`: The classification head name.
 * 
 */
reactor ImageClassifier extends MLReactor {
  preamble {=
    from tflite_support.task import vision, core, processor

    def collect_results(self, classifications):
        results_list = [
            {
                "index": classification.categories[0].index if classification.categories else None,
                "label": classification.categories[0].category_name if classification.categories else None,
                "score": classification.categories[0].score if classification.categories else None,
                "head": classification.head_name
            }
            for classification in classifications if classification
        ]
        return results_list;
  =}

  reaction(startup) {=
    if self.model == "":
        self.debug and print("[IMAGE CLASSIFIER] Error: Please provide a valid model path")
        lf.request_stop()
    else:
        classification_options = self.processor.ClassificationOptions(
            max_results=self.max_results, score_threshold=self.score_threshold)
        options = self.vision.ImageClassifierOptions(base_options=self.base_options, classification_options=classification_options)
        self.executor = self.vision.ImageClassifier.create_from_options(options)
  =}

  reaction(input_data) -> results, inference_time {=
    if input_data.is_present and input_data.value is not None:
        # Classify the input image and get the result.
        self.tensor_input = self.vision.TensorImage.create_from_array(input_data.value)
        start = lf.time.physical()
        result = self.executor.classify(self.tensor_input)
        end = (lf.time.physical() - start) / 1000000
        results.set(self.collect_results(result.classifications))
        inference_time.set(end)
    else:
        results.set([])
        inference_time.set(0)
  =}

  reaction(shutdown) {=
    self.debug and print("[IMAGE CLASSIFIER] Shutting down ImageClassifier reactor")
  =}
}

/**
 * @brief The `ImageSegmenter` reactor extends the `MLReactor` to perform image segmentation tasks 
 * using a TensorFlow Lite model. It processes input images and produces segmentation results 
 * along with inference time.
 * 
 * This reactor leverages the TensorFlow Lite Support Library for image segmentation, offering configurable 
 * options such as model path and output type. It handles input images efficiently, producing segmentation masks 
 * and relevant results.
 * 
 * Args:
 *  - `model` (str): The path to the TensorFlow Lite model file. Required.
 *  - `max_results` (int): The maximum number of segmentations results to return.
 *  - `score_threshold` (float): The minimum score required to include a segmentation in the results.
 *  - `num_threads` (int): Number of CPU threads to use for inference.
 *  - `enable_edgetpu` (bool): Whether to use the Edge TPU for inference.
 *  - `debug` (bool): Enables debug logging for operations. Defaults to true.
 * 
 * Inputs:
 *  - `input_data`: The input image data to be segmented.
 * 
 * Outputs:
 *  - `results`: A list of segmentation results, each containing:
 *    - `segment`: The segmented image or mask.
 *  - `inference_time`: The time taken for inference in milliseconds.
 * 
 * Reactions:
 *  - `startup`: 
 *    - Initializes the image segmenter with the provided model path and segmentation options.
 *    - Validates that a model path is provided; otherwise, logs an error and requests termination.
 *  - `input_data`:
 *    - If input data is present, loads the data into the segmenter's input tensor and runs the segmentation.
 *    - Captures the time taken for inference and outputs it through `inference_time`.
 *    - Collects and formats the segmentation results using the `collect_results` method, then outputs them through `results`.
 *    - Logs an error and requests termination if input data is missing.
 * 
 * Helper Functions:
 *  - `collect_results`: Processes and formats segmentation results into a list of dictionaries. Each entry contains:
 *    - `segment`: The segmented image or mask resulting from the segmentation operation.
 * 
 */
reactor ImageSegmenter extends MLReactor {
  preamble {=
    from tflite_support.task import vision, core, processor

    def collect_results(self, segmentations):
        results_list = [
            {
                "segment": segment
            }
            for segment in segmentations if segment
        ]
        return results_list;
  =}

  reaction(startup) {=
    if self.model == "":
        self.debug and print("[IMAGE SEGMENTER] Error: Please provide a valid model path")
        lf.request_stop()
    else:
        segmentation_options = self.processor.SegmentationOptions(
            output_type=self.processor.SegmentationOptions.output_type.CATEGORY_MASK)
        options = self.vision.ImageSegmenterOptions(
            base_options=self.base_options, segmentation_options=segmentation_options)
        self.executor = self.vision.ImageSegmenter.create_from_options(options)
  =}

  reaction(input_data) -> results, inference_time {=
    if input_data.is_present and input_data.value is not None:
        # Classify the input image and get the result.
        self.tensor_input = self.vision.TensorImage.create_from_array(input_data.value)
        start = lf.time.physical()
        result = self.executor.segment(self.tensor_input)
        end = (lf.time.physical() - start) / 1000000
        results.set(self.collect_results(result.segmentations))
        inference_time.set(end)
    else:
        results.set([])
        inference_time.set(0)
  =}
}

/**
 * @brief The `ObjectDetector` reactor extends the `MLReactor` to perform object detection tasks 
 * using a TensorFlow Lite model. It processes input images to detect objects and provides results 
 * with bounding box coordinates, labels, and confidence scores along with inference time.
 * 
 * This reactor leverages the TensorFlow Lite Support Library for object detection, allowing for configurable 
 * options like model path, detection score threshold, and maximum number of results.
 * 
 * Args:
 *  - `model` (str): The path to the TensorFlow Lite model file. Required.
 *  - `max_results` (int): The maximum number of detections results to return.
 *  - `score_threshold` (float): The minimum score required to include a detection in the results.
 *  - `num_threads` (int): Number of CPU threads to use for inference.
 *  - `enable_edgetpu` (bool): Whether to use the Edge TPU for inference.
 *  - `debug` (bool): Enables debug logging for operations. Defaults to true.
 * 
 * Inputs:
 *  - `input_data`: The input image data in which objects will be detected.
 * 
 * Outputs:
 *  - `results`: A list of detection results, each containing:
 *    - `index`: The index of the detected category.
 *    - `label`: The name of the detected category.
 *    - `box`: The bounding box coordinates for the detected object.
 *    - `score`: The confidence score of the detection.
 *  - `inference_time`: The time taken for inference in milliseconds.
 * 
 * Reactions:
 *  - `startup`: 
 *    - Initializes the object detector with the provided model path and detection options.
 *    - Validates that a model path is provided; otherwise, logs an error and requests termination.
 *  - `input_data`:
 *    - If input data is present, loads the data into the detector's input tensor and runs the object detection.
 *    - Captures the time taken for inference and outputs it through `inference_time`.
 *    - Collects and formats the detection results using the `collect_results` method, then outputs them through `results`.
 *    - Logs an error and requests termination if input data is missing.
 * 
 * Helper Functions:
 *  - `collect_results`: Processes and formats detection results into a list of dictionaries. Each entry contains:
 *    - `index`: The index of the detected category.
 *    - `label`: The name of the detected category.
 *    - `box`: The bounding box coordinates for the detected object.
 *    - `score`: The confidence score of the detection.
 * 
 */
reactor ObjectDetector extends MLReactor {
  preamble {=
    from tflite_support.task import vision, core, processor

    def collect_results(self, detections):
        results_list = [
            {
                "index": detection.categories[0].index if detection.categories else None,
                "label": detection.categories[0].category_name if detection.categories else None,
                "box": detection.bounding_box,
                "score": detection.categories[0].score if detection.categories else None
            }
            for detection in detections if detection
        ]
        return results_list;
  =}

  reaction(startup) {=
    if self.model == "":
        self.debug and print("[OBJECT DETECTOR] Error: Please provide a valid model path")
        lf.request_stop()
    else:
        detection_options = self.processor.DetectionOptions(
            max_results=self.max_results, score_threshold=self.score_threshold)
        options = self.vision.ObjectDetectorOptions(base_options=self.base_options, detection_options=detection_options)
        self.executor = self.vision.ObjectDetector.create_from_options(options)
  =}

  reaction(input_data) -> results, inference_time {=
    if input_data.is_present and input_data.value is not None:
        # Detect objects in the input image and get the result.
        self.tensor_input = self.vision.TensorImage.create_from_array(input_data.value)
        start = lf.time.physical()
        result = self.executor.detect(self.tensor_input)
        end = (lf.time.physical() - start) / 1000000
        results.set(self.collect_results(result.detections))
        inference_time.set(end)
    else:
      results.set([])
      inference_time.set(0)
  =}

  reaction(shutdown) {=
    self.debug and print("[OBJECT DETECTOR] Shutting down ObjectDetector reactor")
  =}
}

# TODO (?)
// reactor ImageSearcher {
// }

# TODO (?)
// reactor ImageEmbedder {
// }