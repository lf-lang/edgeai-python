/**
 * @file NLP.lf
 * @brief File containing Library of Reactor Components for Natural Language Processing (NLP) tasks. This library
 * provides a set of reusable components for performing various NLP tasks, such as text classification, Q&A, and
 * text search.
 *
 * @author Vincenzo Barbuto
 */
target Python

import NLPReactor from "../lib/private/AbstractReactors.lf"

/**
 * @brief The `NLClassifier` reactor extends the `NLPReactor` to perform natural language classification 
 * using a TensorFlow Lite model, supporting both BERT-based and traditional classifiers.
 * 
 * This reactor leverages the TensorFlow Lite Support Library for text classification and supports 
 * two types of classifiers: traditional and BERT-based, depending on the `BERT_based` argument.
 * 
 * Args:
 *  - `model` (str): The path to the TensorFlow Lite model file. Required.
 *  - `BERT_based` (bool): A flag indicating whether the classifier is BERT-based. Defaults to `True`.
 *  - `debug` (bool): Enables debug logging for operations. Defaults to `true`.
 * 
 * Inputs:
 *  - `input_data`: The input text data to be classified.
 * 
 * Outputs:
 *  - `results`: A list of classification results, each containing:
 *    - `name`: The name of the classified category.
 *    - `score`: The confidence score of the classification.
 *  - `inference_time`: The time taken for inference in milliseconds.
 * 
 * Reactions:
 *  - `startup`: 
 *    - Initializes the classifier based on whether it is BERT-based or not.
 *    - Validates that a model path is provided; otherwise, logs an error and requests termination.
 *  - `input_data`:
 *    - If input data is present, loads the data and runs the classification.
 *    - Captures the time taken for inference and outputs it through `inference_time`.
 *    - Collects and formats the classification results using the `collect_results` method, then outputs them through `results`.
 *    - Logs an error and requests termination if input data is missing.
 * 
 * Helper Functions:
 *  - `collect_results`: Processes and formats classification results into a list of dictionaries. Each entry contains:
 *    - `name`: The name of the classified category.
 *    - `score`: The confidence score of the classification.
 * 
 */
reactor NLClassifier(BERT_based=True) extends NLPReactor {
  preamble {=
    from tflite_support.task import text

    def collect_results(self, result):
        results_list = []
        for classification in result.classifications[0].categories:
            results_list.append({
                "name": classification.category_name,
                "score": classification.score
            })
        return results_list
  =}

  reaction(startup) {=
    if self.model == "":
        self.debug and print("[NL CLASSIFIER] Error: Please provide a valid model path")
        lf.request_stop()
    elif self.BERT_based:
        self.executor = self.text.BertNLClassifier.create_from_file(self.model)
    else:
      self.executor = self.text.NLClassifier.create_from_file(self.model)
  =}

  reaction(input_data) -> results, inference_time {=
    if input_data.is_presenr and input_data.value is not None:
        start = lf.time.physical()
        result = self.executor.classify(input_data.value)
        end = (lf.time.physical() - start) / 1000000
        results.set(self.collect_results(result))
        inference_time.set(end)
  =}

  reaction(shutdown) {=
    self.debug and print("[NL CLASSIFIER] Shutting down NLClassifier reactor")
  =}
}

/**
 * @brief The `BertQuestionAnswer` reactor extends the `NLPReactor` to perform question answering 
 * using a BERT-based model. It loads a specified model and context file and returns answers to questions 
 * based on the given context.
 * 
 * This reactor uses the TensorFlow Lite Support Library for text-based question answering using the BERT 
 * model, processing input questions with respect to a provided context file.
 * 
 * Args:
 *  - `model` (str): The path to the TensorFlow Lite model file. Required.
 *  - `context_file` (str): The path to a file containing the context used for question answering. Required.
 *  - `debug` (bool): Enables debug logging for operations. Defaults to `true`.
 * 
 * Inputs:
 *  - `input_data`: The input question text to be answered based on the provided context.
 * 
 * Outputs:
 *  - `results`: A list of answer results, each containing:
 *    - `index`: The index of the answer.
 *    - `answer`: The text of the answer.
 *  - `inference_time`: The time taken for inference in milliseconds.
 * 
 * Reactions:
 *  - `startup`: 
 *    - Initializes the BERT question-answering model from the provided model path.
 *    - Reads the context from the provided context file.
 *    - Validates that both model and context paths are provided; otherwise, logs an error and requests termination.
 *  - `input_data`:
 *    - If input data (question) is present, runs the question-answering process using the loaded context.
 *    - Captures the time taken for inference and outputs it through `inference_time`.
 *    - Collects and formats the answer results using the `collect_results` method, then outputs them through `results`.
 *  - `shutdown`:
 *    - Logs the shutdown of the reactor.
 * 
 * Helper Functions:
 *  - `collect_results`: Processes and formats the question-answering results into a list of dictionaries. Each entry contains:
 *    - `index`: The index of the answer.
 *    - `answer`: The text of the answer.
 * 
 */
reactor BertQuestionAnswer(context_file="") extends NLPReactor {
  state context

  preamble {=
    from tflite_support.task import text

    def collect_results(self, result):
      results_list = []
      for i, answer in enumerate(result.answers):
        results_list.append({
          "index": i,
          "answer": answer.text,
        })
      return results_list
  =}

  reaction(startup) {=
    if self.model == "":
      self.debug and print("[BERT Q&A] Error: Please provide a valid model path")
      lf.request_stop()
    elif self.context_file == "":
      self.debug and print("[BERT Q&A] Error: Please provide a valid context path")
      lf.request_stop()
    else:
      self.executor = self.text.BertQuestionAnswerer.create_from_file(self.model)
      file = open(self.context_file, "r")
      self.context = file.read()
  =}

  reaction(input_data) -> results, inference_time {=
    if input_data.is_present and input_data.value is not None:
      start = lf.time.physical()
      result = self.executor.answer(self.context, input_data.value)
      end = (lf.time.physical() - start) / 1000000
      results.set(self.collect_results(result))
      inference_time.set(end)
  =}

  reaction(shutdown) {=
    self.debug and print("[BERT Q&A] Shutting down BertQuestionAnswer reactor")
  =}
}

# TODO
// reactor TextSearcher {
// }

# TODO
// reactor TextEmbedder {
// }
